## Introduction

In the 1940s, Claude Shannon showed that a precise mathematical definition of information arises directly from the axioms of probability. Since then, information theory has served as a foundation for the digital world we live in today by defining the limits of data compression and the reliable transmission of data across noisy networks. 

In addition, the mathematical tools it provides have
found numerous applications in diverse areas such as statistics, machine learning, cryptography, quantum computing, biology, and many others.

## Main Ideas
- **Layman terms**:Information is "Knowledge obtained about something unknown" 
- Information theory formalized this intuition into something mathematically precise, by **defining it in terms of probability**.
- Information theory shows that the randomness of the message, as characterized by the probability distribution over potential messages, is in fact the only thing that matters for figuring out how to transmit information. The actual content of the messages is irrelevant. 
- The fundamental unit of information content is the bit.

### Key Problems 
1. **Source Coding**(Data Compression)
	1. Lossless Compression
	2. Lossy Compression
2. **Channel Coding**(Data transmission) 
	1. Perfect Transmission
	2. Transmission with Errors

## Concepts
1. [[Entropy and Information]]
2. [[Entropy and Data Compression]]
3. [[Mutual Information]]
4. 