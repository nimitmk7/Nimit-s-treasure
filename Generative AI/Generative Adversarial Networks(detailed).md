## Premise

### Why do we use max likelihood ?
In maximum likelihood, we find the optimal distribution parameters $\theta$ for a sample of data points such that:
$$\hat{\theta} = \underset{\theta}{\operatorname{argmax}} \sum_{i=1}^{M} \log p_{\theta}(x_{i})
$$
where
$x_1, x_{2},\cdots, x_{M} \sim p_{data}(x)$

But why use MLE ?
1. **Optimal statistical efficiency**. 
	1. Under the assumption of sufficient model capacity, such that there exists a unique $\theta^{*} ∈ M$ that satisfies $p_{θ^∗} = p_{data}$, the convergence of $\hatθ$ to $θ^∗$ when $M → \infty$ is the “fastest” among all statistical methods when using maximum likelihood training.
2. **Higher likelihood = better lossless compression**. 


### Towards likelihood-free learning

But is the likelihood a good indicator of the quality of samples generated by the model?

Not necessarily. There are cases in which achieving high likelihood might not necessarily be correlated with achieving good sample quality. 

For example, if you want to train a generative model over images, its possible to:
1. Construct models that would give you high likelihood and terrible samples in terms of quality.
2. Train models that have very good sample quality(produce very realistic images), but they have terrible likelihoods at the same time. 

**Case 1:** Optimal generative model will give best **sample quality** and highest test **log-likelihood**. 
- For imperfect models, achieving high log-likelihoods might not always imply good sample quality, and vice versa. 

**Case 2:** Great test log-likelihoods, poor samples. 
E.g. For a discrete noise mixture model, $p_{\theta} = 0.01p_{data}(x) + 0.99p_{noise(x)}$, 
- 99% of the samples are just noise(most samples are poor)
- Taking logs, we get a lower bound. $$\log p_{\theta}(x) = \log[0.01p_{data}(x) + 0.99p_{noise(x)}] \ge \log 0.01p_{data}(x) - \log 100$$
-  For expected log-likelihoods, we know that, 
	- Lower bound $$E_{p_{data}}[\log {p_{\theta}}(x)] \ge E_{p_{data}}[\log p_{data(x)}] - \log 100$$
	- Upper bound(via non-negativity of $D_KL(p_{data}|| \space p_{\theta}) \ge 0)$): $$E_{p_{data}}[\log p_{data}(x)] \ge E_{p_{data}}[\log p_{\theta}(x)]$$
	- As we increase the dimension $n$ of $x = (x_1, · · · , x_n)$, absolute value of $\log p_{data}(x) = \sum_{n}^{i=1} \log p_{\theta}(x_{i} |x_{<i})$ increases proportionally to $n$ but log 100 remains constant.
	- Hence, likelihoods are great in high dimensions. $$E_{p_{data}}[\log {p_{\theta}}(x)] \sim E_{p_{data}}[\log p_{data(x)}]$$


**Case 3**: Great samples, poor test log-likelihoods. E.g. Memorizing training set.
- Samples look exactly like the training set(cannot do better!)
- Test set will have zero probability assigned(cannot do worse!)
- The above case suggests that it might be useful to disentangle sample quality and likelihoods. 

**Likelihood-free learning** consider alternative training objectives that do not depend directly on a likelihood function.

## Comparing distributions via samples

Given a finite set of samples from 2 distributions $S_1 = \{x \sim P\}$ and $S_2 = \{x \sim Q\}$, how can we tell if these samples are from the same distribution?
(i.e., $P = Q?$)

![[Pasted image 20240619120239.png]]

### Two-sample tests

- Given  $S_1 = \{x \sim P\}$ and $S_2 = \{x \sim Q\}$, a **two-sample test** considers the following hypotheses:
	- Null hypothesis: $H_0: P = Q$
	- Alternative hypothesis: $H_1: P \neq Q$
- Test Statistic compares $S_1$ and $S_2$. For example: difference in means, variances of the two sets of samples
	- $T(S_1, S_2) = \left|\frac{1}{|S_1|}\sum_{x \in S_{1}}x - \frac{1}{|S_2|}\sum_{x \in S_{2}}x \right|$
- If $T$ is larger than a threshold $\alpha$, then reject $H_0$ otherwise say $H_0$ is consistent with observation.
- **Key Observation**: Test statistic is **likelihood-free** since it does not involve the densities $P$ or $Q$(only samples).

### Generative modeling with two-sample tests

- A priori we assume direct access to $S_{1} = D = \{x ∼ p_{data}\}$
- In addition, we have a model distribution $p_{\theta}$.
- Assume that the model distribution permits efficient sampling(e.g. directed models). Let $S_{2} = \{x \sim p_{\theta}\}$.
- **Alternative notion of distance between distributions**: Train the generative model to minimize a two-sample test objective between $S_1$ and $S_{2}$

![[Pasted image 20240619141058.png]]

### Two-sample test via a Discriminator
- Finding a good two-sample test objective in high dimensions is hard. 
![[Pasted image 20240619141319.png]]
a. Same shape, different mean
b. Same mean, different variance
c. Same mean, same variance, but different shape/densities

We might match one test statistic, but not match a lot others which actually matter a lot in practice. 

- In the generative model setup, we know that $S_1$ and $S_2$ come from different distributions $p_{data}$ and $p_{\theta}$ respectively 
- **Key idea**: Learn a statistic to automatically identify in what way the two sets of samples $S_1$ and $S_2$ differ from each other 
- How? Train a classifier (called a **discriminator**)!

![[Pasted image 20240619142311.png]]
- Two-sample test via a Discriminator
	- Any binary classifier $D_\phi$ (e.g. neural network) which tries to distinguish “real”$(y=1)$ samples from the dataset and “fake”$(y=0)$ samples generated from the model.
	- Test statistic: -loss of classifier. 
		- Low loss, real and fake samples are easy to distinguish(different). 
		- High loss, real and fake samples are hard to distinguish(similar).
	- Goal: Maximize the 2-sample test statistic(in support of the alternative hypothesize $p_{data} = p_{θ}$), or equivalently minimize classification loss.
- Training objective for discriminator: $$\begin{eqnarray}
\underset{D_{\theta}}{\operatorname{max}}V(p_{\theta}, D_{\phi}) = E_{x \sim p_{data}} [\log D_{\phi}(x)] + E_{x\sim p_{\theta}} [\log(1 - D_{\phi}(x))] \\ \approx \sum_{x \in S_{1}} \log D_{\phi}(x) + \sum_{x \in S_{2}} [\log (1- D_{\phi}(x))] \end{eqnarray}$$
- For a fixed generative model $p_{\theta}$, the discriminator is performing binary classification with the cross entropy objective:
	- Assign probability 1 to true data points $x \sim p_data (in set $S_1$)
	- Assign probability 0 to fake samples $x \sim p_θ$ (in set $S_{2}$)
- Optimal Discriminator
	$$D_{\theta}^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_{\theta}(x)}$$
- Sanity check: if $p_{\theta} = p_{data}$, classifier cannot do better than chance $(D_{θ}^* (x) = 1/2)$, $x$ is equally likely to be in both distributions.

## Generative Adversarial Networks
- A two player minimax game between a **generator** and a **discriminator**. 

### Generator
![[Pasted image 20240619145438.png]]
- Directed, latent variable model with a deterministic mapping between $z$ and $x$ given by $G_{theta}$. 
	- Sample $z \sim p(z)$, where $p(z)$ is a simple prior, e.g. Gaussian.
	- Set $x = G_{theta}(z)$.
- Similar to a flow model, but mapping $G_{theta}$ need not be invertible.
- Distribution $p_{theta}(x)$ over $x$ is implicitly defined(no likelihood!).
- Minimizes a 2-sample test objective(in support of the null hypothesis $p_{data} = p_{θ}$).

#### Training Objective
$$\begin{eqnarray}
\underset{G}{\operatorname{min}} \underset{D}{\operatorname{max}} V(G, D) = E_{x \sim p_{data}} [\log D(x)] + E_{x\sim p_{G}} [\log(1 - D(x))] \\ \approx \sum_{x \in S_{1}} \log D_{\phi}(x) + \sum_{x \in S_{2}} [\log (1- D_{\phi}(x))] \end{eqnarray}$$
- For the optimal discriminator $D_G^*(\cdot)$ , we have $$\begin{eqnarray}
V(G, D_G^*(x)) = E_{x\sim p_{data}}\left[\log \frac{p_{data}(x)} {p_{data}(x)+p_{G}(x)} \right] + E_{x∼p_{G}}\left[\log \frac{p_{G}(x)} {p_{data}(x)+p_{G}(x)} \right] \\
= E_{x\sim p_{data}}\left[\log \frac{p_{data}(x)} {\frac{p_{data}(x)+p_{G}(x)}{2}} \right] + E_{x∼p_{G}}\left[\log \frac{p_{G}(x)} {\frac{p_{data}(x)+p_{G}(x)}{2}} \right] -\log 4 \\
= D_{KL}\left[p_{data}, {\frac{p_{data}+p_{G}}{2}} \right] + D_{KL} \left[p_{G}, {\frac{p_{data}+p_{G}}{2}} \right] -\log 4 \\ 
= 2 D_{JSD}[p_{data}, p_{G}] - \log {4} \\
\end{eqnarray} $$
##### Jensen Shannon Divergence
- Also called as the symmetric KL divergence.  $$D_{JSD}[p, q] = \frac{1}{2}\left(D_{KL}\left[p, {\frac{p+q}{2}} \right] + D_{KL} \left[q, {\frac{p+q}{2}} \right]\right)$$
- Properties
	- $D_{JSD}[p,q] \ge 0$
	- $D_{JSD}[p,q] = 0$ iff $p=q$
	- $D_{JSD}[p,q] = D_{JSD}[q,p]$
	- $\sqrt{D_{JSD}[p,q]}$ satisfies triangle inequality → Jenson Shannon Distance
- Optimal Generator for the JSD/Negative Cross Entropy GAN $$p_{G} = p_{data}$$
- For the optimal discriminator $D_{G∗}^*(·)$ and generator $G^*(·)$, we have $$V(G^∗ , D_{G∗}^* (x)) = − \log 4$$
## Training Algorithm
1. Sample mini-batch of $m$ training points $x^{(1)}, x^{(2)}, \dots ,x^{(m)}$ from $D$.
2. Sample mini-batch of $m$ noise vectors $z^{(1)}, z^{(2)}, \dots ,z^{(m)}$ from $p_z$.
3. Pass the noise vectors through $G$ to get $m$ fake samples.
4. Update the discriminators parameters $\phi$ by stochastic gradient **ascent**. $$\nabla_{\phi} V(G_{\theta}, D_{\phi}) = \frac{1}{m}\nabla _{\phi}\sum_{i=1}^{m}[\log D_{\phi}(x^{(i)}) + \log(1 - D_{\phi}(G_{\theta}(z^{(i)})))]$$
5. Update the generator parameters $\theta$ by stochastic gradient **descent**. $$\nabla_{\phi} V(G_{\theta}, D_{\phi}) = \frac{1}{m}\nabla _{\phi}\sum_{i=1}^{m} \log(1 - D_{\phi}(G_{\theta}(z^{(i)})))$$
6. Repeat for a fixed number of epochs. 
#### Alternating Optimization in GANs

![[Pasted image 20240619163556.png]]
## Pros
1. Loss only requires samples from $p_\theta$. No likelihood needed!
2. Lots of flexibility for the neural network architecture, any $G_\theta$ defines a valid sampling procedure.
3. Fast sampling, single forward pass.
## Cons
1. Difficult to train in practice

## Practical Challenges
1. Unstable optimization
2. Mode collapse: Generator collapses to one or few samples.
3. Evaluation

## References
1. https://deepgenerativemodels.github.io/assets/slides/cs236_lecture9.pdf 
2. 





