## What is Generative Modeling?

> Generative modeling is a branch of machine learning that involves training a model to produce new data that is similar to a given dataset.

**Example**:
Suppose we have a dataset containing photos of horses. We can train a generative model on this dataset to capture the rules that gov‐ ern the complex relationships between pixels in images of horses. Then we can sam‐ ple from this model to create novel, realistic images of horses that did not exist in the original dataset. 

![[Pasted image 20240531165200.png]]

In order to build a generative model, we require a dataset consisting of many examples of the entity we are trying to generate, which is **training data**, and one such data point is **observation**.

Each observation consists of many features. For an image generation problem, the features are usually the individual pixel values; for a text generation problem, the features could be individual words or groups of letters. 

**Goal**: Build a model that can generate new sets of features that look as if they have been created using the same rules as the original data. 

==A generative model must also be probabilistic rather than deterministic==, because we want to be able to sample many different variations of the output, rather than get the same output every time.

Therefore, a generative model must include a random component that influences the individual samples generated by the model. 

### Generative vs Discriminative Modeling

When performing discriminative modeling, each observation in the training data has a label. For a binary classification problem such as our artist discriminator, Van Gogh paintings would be labeled 1 and non–Van Gogh paintings labeled 0.

![[Pasted image 20240531165814.png]]
Our model then learns how to discriminate between these 2 groups and outputs the probability that a new observation has label 1 - i.e. that it was painted by Van Gogh.

In contrast, ==generative modeling doesn’t require the dataset to be labeled== because it concerns itself with generating entirely new images, rather than trying to predict a label of a given image.

> *Discriminative modeling* estimates $p(y|x)$. 
> 
> That is, discriminative modeling aims to model the probability of a label $y$ given some observation $x$.

> *Generative modeling* estimaters $p(x)$.
> 
> That is, generative modeling aims to model the probability of observing an observation $x$. Sampling from this distribution allows us to generate new observations.

> [!INFO] Conditional Generative Models
>  Note that we can also build a generative model to model the conditional probability $p(x|y)$ → the probability of seeing an observation $x$ with a label $y$.  
>  
>  For example, if our dataset contains different types of fruit, we could tell our generative model to specifically generate an image of an apple.

## Toy Generative Model

### Hello World!

Generative modeling game in 2D: 
Given a set of points $X$ in the figure generated by an unknown rule $p_{data}$ , choose a different point $x=(x_1, x_2)$ in the space that looks like it has been generated by the same rule. 

![[Pasted image 20240531200526.png]]

Where did you choose? You probably used your knowledge of the existing data points to construct a mental model, $p_{model}$, of whereabouts in the space the point is more likely to be found. In this respect, $p_{model}$ is an estimate of pdata. 

Perhaps you decided that $p_{model}$ should look like the next figure—a rectangular box where points may be found, and an area outside of the box where there is no chance of finding any points.

![[Pasted image 20240531200709.png]]
To generate a new observation, you can simply choose a point at random within the box, or more formally, sample from the distribution $p_{model}$. Congratulations, you have just built your first generative model! 

You have used the training data (the black points) to construct a model (the orange region) that you can easily sample from to generate other points that appear to belong to the training set.

### Generative Modeling Framework
- Dataset of observations $X$.
- Unknown distribution generating the data: $p_{data}$
- Our model to be built to mimic it: $p_{model}$
- If we achieve that, we can sample from $p_{model}$ to generate observations that appear to have been drawn from $p_{data}$.
- Desirable properties of $p_{model}$: 
	-  *Accuracy* : If $p_{model}$ is high for a generated observation, it should look like it has been drawn from $p_{data}$. If $p_{model}$ is low for a generated observation, it should not look like it has been drawn from $p_{data}$. 
	- *Generation*: It should not be possible to easily sample a new observation from $p_{model}$.
	- *Representation*: It should be possible to understand how different high-level features are represented by $p_{model}$. 

True data generating distribution $p_{data}$: 

![[Pasted image 20240601071103.png]]
The data-generating rule is simply a uniform distribution over the land mass of the world, with no chance of finding a point in the sea.

Clearly, our model, $p_{model}$, is an oversimplification of $p_{data}$. 












