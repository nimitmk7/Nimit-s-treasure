
In-context learning (ICL) is a specific method of prompt engineering where demonstrations of the task are provided to the model as part of the prompt (in natural language).

With ICL, you can use off-the-shelf large language models (LLMs) to solve novel tasks without the need for [fine-tuning](https://www.hopsworks.ai/dictionary/fine-tuning-llms). ICL can also be combined with fine-tuning for more powerful LLMs.

A smalls set of examples are presented within the context(the prompt) at inference time. LLMs trained on sufficient data exhibit ICL, even though they are trained only with the objective of next token prediction.
## Few-shot prompting

Few-shot prompting is a specific technique within the broader framework of in-context learning. It involves providing the model with a small number of examples (typically less than 10) demonstrating the desired task.


## References
1. https://www.hopsworks.ai/dictionary/in-context-learning-icl
2. 


