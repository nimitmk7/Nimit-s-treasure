
Given that we have a good $p(y \space| \space x)$, sampling is easy for autoregressive models:

## Standard Sampling
- While output is not EOS:
	- Sample next word _from_ $P(\cdot \space| \space \text{prefix},\text{input}; \theta)$
	- Append the word to prefix

### Problem
Standard sampling often produces **non-sensical** sentences. 

### Fix
**Idea**: Modify the learned distribution $p_θ$ before sampling to avoid bad generations.

## Tempered Sampling
**Intuition:** Concentrate probability on highly likely sequences

### Temperature
It is a technique used to ==redistribute the probabilities== of the possible values. 
1. Reduce the probabilities of common tokens
2. Increase the probabilities of rare tokens

Temperature is a constant used to adjust the logits before the softmax transformation. Logits are divided by temperature. 

Given a classification layer(Vocabulary size), which outputs a output logit vector $[x_{1}, x_{2}, \dots, x_{N}]$.

![[Pasted image 20241001152138.png]]
==While larger logits correspond to higher probabilities, the logits don’t represent the probabilities==. Logits don’t sum up to one. Logits can even be negative, while probabilities have to be non-negative. To convert logits to probabilities, a **softmax layer** is often used.

In a standard layer, the probability for the $i^{th}$ token, $p_i$ is computed as follows:
$$p_{i} = softmax\left( \frac {e^{x_{i}}}{\sum_{j}e^{x_{j}}} \right)$$

For a given temperature of $T$, ==the adjusted logit for the $i^{th}$ token is $\frac{x_{i}}{T}$==. Softmax is then applied on this adjusted logit instead of on $x_i$.
### Example
 Imagine that we have a model that has only two possible outputs: A and B. The logits computed from the last layer are `[1, 3]`. The logit for A is 1 and B is 3.

- Without using temperature, equivalent to temperature = 1, the softmax probabilities are `[0.12, 0.88]`. The model picks B 88% of the time.
- With temperature = 0.5, the probabilities are `[0.02, 0.98]`. The model picks B 98% of the time.
- With temperature = 2, the probabilities are `[0.27, 0.73]`. The model picks B 73% of the time.

The graph below shows the softmax probability for token B at different temperatures. As the temperature gets closer to 0, the probability that the model picks token B becomes closer to 1. In our example, for temperature below 0.1, the model almost always outputs B.


![[Pasted image 20241001153046.png]]
### Effect
Higher temperature → Less likely that model picks the most obvious value → Model’s outputs are more creative and less coherent(consistent and reliable)

Lower temperature → More likely that model picks the most obvious value → Model’s outputs are more consistent but potentially boring

### Practical Considerations
1. Model providers ==typically limit temperature to be between 0 and 2==. (It makes the distribution more peaky).
2. If you own your model, you can use any non-negative temperature.
3. ==A temperature of 0.7 is often recommended for creative use cases==, as it balances creativity and determinism, but you should experiment and find the temperature that works best for you.

#### Setting temperature as zero
It’s common practice to set the temperature to 0 for the model’s outputs to be more consistent. Technically, temperature can never be 0 – logits can’t be divided by 0. 

In practice, when we set the temperature to 0, the model just picks the token with the value with the largest logit, e.g. performing an `argmax`, without doing the logit adjustment and softmax calculation.

#### Logprobs
A common debugging technique when working with an AI model is looking at the probabilities this model computes for given inputs. For example, ==if the probabilities look random, the model hasn’t learned much.== 

OpenAI returns probabilities generated by their models as _[logprobs](https://cookbook.openai.com/examples/using_logprobs)_. Logprobs, short for log probabilities, are probabilities in the log scale. 

==Log scale is preferred when working with a neural network’s probabilities because it helps reduce the underflow problem==. 

A language model can work with a vocabulary size of 100,000, which means the probabilities for many of the tokens can be too small to be represented by a machine. The small numbers might be rounded down to 0. Log scale helps reduce this problem.

## Truncated Sampling
It is another way to focus on highly likely sequences: ==truncate the tail of the distribution==
### Top-k sampling
After the model has computed the logits, we pick the **top k** logits and perform **softmax** over these **top k** logits only. 

#### Advantage
It helps reduce the computational workload without sacrificing too much of the model’s response diversity. **How?**

Recall that to compute the probability distribution over all possible values, a softmax layer is used. Softmax requires two passes over all possible values: 
1. To perform the exponential sum $\sum_j e^{x_j}$ , and 
2. To perform $\frac{e^{x_i}}{\sum_j e^{x_j}}$ for each value. 

For a language model with a large vocabulary, this process is computationally expensive. Restricting the calculation to top k words speeds up the computation.
#### Effect
A smaller k value makes the text more predictable but less interesting, as the model is limited to a smaller set of likely words.

A larger k value gives a more diverse output but with the possibility of it being degenerate.

### Top-p sampling
#### Premise
In top-k sampling, the number of values considered is fixed to k. However, this number should change depending on the situation. 

For example, given the prompt `Do you like music? Answer with only yes or no.`, the number of values considered should be two: `yes` and `no`. 

Given the prompt `What's the meaning of life?`, the number of values considered should be much larger.

**Top-p**, also known as **nucleus sampling**, allows for a more dynamic selection of values to be sampled from. 

#### Method
In top-p sampling, the model ==sums the probabilities of the most likely next values in descending order and stops when the sum reaches p==. Only the values within this cumulative probability are considered.

Common values for top-p (nucleus) sampling in language models typically range from 0.9 to 0.95. A top-p value of 0.9, for example, means that the model will consider the smallest set of values whose cumulative probability exceeds 90%.

Let’s say the probabilities of all tokens are as shown in the image below. If top_p = 90%, only `yes` and `maybe` will be considered, as their cumulative probability is greater than 90%. If top_p = 99%, then `yes`, `maybe`, and `no` are considered.

![[Pasted image 20241001163449.png | 400]]
#### Pros and Cons
Unlike top-k, top-p ==doesn’t necessarily reduce the softmax computation load==.

Its benefit is that because it focuses on only the set of most relevant values for each context, it ==allows outputs to be more contextually appropriate==. 

In theory, there doesn’t seem to be a lot of benefits to top-p sampling. However, in practice, top-p has proven to work well, causing its popularity to rise.



## References
1. https://huyenchip.com/2024/01/16/sampling.html